{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "!pip install nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# not deps but we need them to use nbdev and run tests\n",
    "from nbdev import * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU and memory management.\n",
    "\n",
    "> Code snippets to help memory management in Google Colab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def versions():\n",
    "  \"Checks if GPU enabled and if so displays device details with cuda, pytorch, fastai versions\"\n",
    "  print(\"GPU: \", torch.cuda.is_available())\n",
    "  if torch.cuda.is_available() == True:\n",
    "    print(\"Device = \", torch.device(torch.cuda.current_device()))\n",
    "    print(\"Cuda version - \", torch.version.cuda)\n",
    "    print(\"cuDNN version - \", torch.backends.cudnn.version())\n",
    "    print(\"PyTorch version - \", torch.__version__)\n",
    "    print(\"fastai version\", fastai.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `!nvidia-smi` to get GPU details and current memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.cuda.empty_cache()` clears GPU cache to free up GPU memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPU RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def show_ram_usage():\n",
    "  import psutil\n",
    "  \"Check overall RAM usage\" \n",
    "  pu_mem = psutil.Process(os.getpid())\n",
    "  print('RAM usage: {} GB'.format(pu_mem_,memory_info([0]/1024 ** 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def purge_mem(vars=None, mem=True):\n",
    "  \"Delete `vars` and recycle CPU memory. Show new RAM if `mem=True`\"\n",
    "  for var in vars:\n",
    "    del var\n",
    "  gc.collect()\n",
    "  if mem: show_ram_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def show_df_mem_usage(df):\n",
    "  \"Check dataframe `df` memory use\"\n",
    "  print('Shape: ', df.shape)\n",
    "  print('Total Mem usage: ', df.memory_usage().sum())\n",
    "  df.memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def reduce_df_mem_usage(df, verbose=True):\n",
    "  \"Reduce size of dataframe `df` columns to minimise memory usage\"\n",
    "  numerics = ['int16', 'int32', 'int64', 'float64']\n",
    "  start_mem = df.memory_usage().sum() / 1024**2    \n",
    "  for col in df.columns:\n",
    "      col_type = df[col].dtypes\n",
    "      if col_type in numerics:\n",
    "          c_min = df[col].min()\n",
    "          c_max = df[col].max()\n",
    "          if str(col_type)[:3] == 'int':\n",
    "              if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                  df[col] = df[col].astype(np.int8)\n",
    "              elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                  df[col] = df[col].astype(np.int16)\n",
    "              elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                  df[col] = df[col].astype(np.int32)\n",
    "              elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                  df[col] = df[col].astype(np.int64)  \n",
    "          else:\n",
    "              if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                  df[col] = df[col].astype(np.float16)\n",
    "              elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                  df[col] = df[col].astype(np.float32)\n",
    "              else:\n",
    "                  df[col] = df[col].astype(np.float64)    \n",
    "  end_mem = df.memory_usage().sum() / 1024**2\n",
    "  if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "  return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
